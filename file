Below is a refactored version of your comparison script that processes each table in batches rather than pulling the entire million-row tables into memory at once. The key changes are:

1. Chunked fetching from Snowflake via cursor.fetchmany().


2. For each Snowflake batch, we build a small WHERE clause to pull only the matching Vertica rows.


3. We immediately write any mismatches for that batch to the Excel report (appending) and then discard the batch.


4. We never hold both full tables in RAM simultaneously—only one batch at a time.



from openpyxl import load_workbook
import os
import pandas as pd
import snowflake.connector
import vertica_python
from dotenv import load_dotenv

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)

load_dotenv()

SCHEMA = os.getenv("ER1_SCHEMA", "ER1")
START_DATETIME = "2020-10-01 00:00:00"
END_DATETIME = "2025-06-24 23:59:59"
CHUNK_SIZE = 10000  # adjust as needed for your memory/throughput trade-off

def compare_table(table, config_df, sf_conn, vt_conn, summary, output_path, first_write):
    sub = config_df[config_df["Table Name"].str.casefold() == table.casefold()]
    if (sub["Flag"].str.casefold() == "stage").any():
        summary.append(f"{table}: Skipped (Stage flagged)")
        return first_write

    # identify key, exclude, and compare columns
    key_cols     = sub[sub["Flag"].str.casefold() == "key"]["Column Name"].tolist()
    exclude_cols = sub[sub["Flag"].str.casefold() == "exclude"]["Column Name"].tolist()
    filter_cols  = sub[sub["Flag"].str.casefold() == "filter"]["Column Name"].tolist()
    filter_col   = filter_cols[0] if filter_cols else None

    # fetch full list of columns just to resolve names & common set
    sf_cols = get_columns(sf_conn, table, dbtype="snowflake")
    vt_cols = get_columns(vt_conn, table, dbtype="vertica")
    all_cols = list(set(sf_cols + vt_cols))

    keys         = resolve_columns(key_cols, all_cols, force_upper=True)
    excludes     = resolve_columns(exclude_cols, all_cols, force_upper=True) if exclude_cols else []
    compare_cols = [c for c in sf_cols if c in vt_cols and c not in keys + excludes]

    # build SELECT list
    select_cols = keys + compare_cols
    col_str = ", ".join(f'"{c}"' for c in select_cols)

    # prepare Excel writer if first batch
    if first_write:
        if os.path.exists(output_path):
            os.remove(output_path)
        book = None
    else:
        book = load_workbook(output_path)

    # Snowflake cursor and query
    sf_cur = sf_conn.cursor()
    base_query = f'SELECT {col_str} FROM "{SCHEMA}"."{table}"'
    if filter_col:
        base_query += f' WHERE "{filter_col}" BETWEEN \'{START_DATETIME}\' AND \'{END_DATETIME}\''

    sf_cur.execute(base_query)

    batch_num = 0
    while True:
        batch = sf_cur.fetchmany(CHUNK_SIZE)
        if not batch:
            break
        batch_num += 1

        # build a small DataFrame for this batch
        df_sf = pd.DataFrame(batch, columns=[d[0].upper() for d in sf_cur.description])
        df_sf = df_sf.add_suffix("_sf")

        # build a dynamic WHERE to fetch matching Vertica rows by key
        key_conds = []
        for _, row in df_sf.iterrows():
            parts = []
            for k in keys:
                val = row[f"{k}_sf"]
                if pd.isna(val):
                    parts.append(f'"{k}" IS NULL')
                else:
                    parts.append(f'"{k}" = \'{val}\'')
            key_conds.append("(" + " AND ".join(parts) + ")")
        vt_where = " OR ".join(key_conds) or "1=0"
        vt_query = f'SELECT {col_str} FROM "{SCHEMA}"."{table}" WHERE {vt_where}'

        # fetch and suffix
        vt_cur = vt_conn.cursor()
        vt_cur.execute(vt_query)
        vt_rows = vt_cur.fetchall()
        df_vt = pd.DataFrame(vt_rows, columns=[d[0].upper() for d in vt_cur.description]).add_suffix("_vt")
        vt_cur.close()

        # merge this batch
        merged = pd.merge(
            df_sf, df_vt,
            left_on=[f"{k}_sf" for k in keys],
            right_on=[f"{k}_vt" for k in keys],
            how='outer'
        )

        # collect mismatches
        mismatches = compare_rows(merged, compare_cols, keys, table)
        if mismatches:
            df_out = pd.DataFrame(mismatches)[["TableName","ColumnName","ValueInVertica","ValueInSnowflake","Key"]]
            # append to Excel
            with pd.ExcelWriter(output_path, engine="openpyxl", mode="w" if first_write else "a") as writer:
                if book:
                    writer.book = book
                    writer.sheets = {ws.title: ws for ws in book.worksheets}
                    start_row = writer.sheets['Sheet1'].max_row
                else:
                    start_row = 0
                df_out.to_excel(writer, index=False, header=first_write, startrow=start_row)
                first_write = False
                book = writer.book  # carry forward for next append

        summary.append(f"{table} batch {batch_num}: processed {len(df_sf)} rows, {len(mismatches)} mismatches")

    sf_cur.close()
    return first_write

What changed and why

CHUNK_SIZE controls how many Snowflake rows you hold in memory at once.

We use cursor.fetchmany() to pull only CHUNK_SIZE rows per loop.

For each small batch, we immediately:

1. Build a tiny Vertica query covering just those keys.


2. Fetch, compare, and write any mismatches to the Excel file.



We then discard the batch’s DataFrames before loading the next.

At no point do we keep the full million-row tables in RAM, so memory usage stays roughly proportional to CHUNK_SIZE rather than total table size.


This should reduce your peak memory footprint from multiple gigabytes down to whatever a single batch plus its merge requires (e.g. a few tens of megabytes). Feel free to tune CHUNK_SIZE up or down depending on the sweet spot between memory use and query overhead.

