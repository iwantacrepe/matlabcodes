Here is your revised 15-day theoretical project report, fully excluding Power BI and Pentaho testing, and focusing purely on Python-based validation between Vertica and Snowflake:


---

Title: Data Validation Between Vertica and Snowflake Using Python


---

Objective:

To build a robust Python-based solution for comparing table schemas and data between Vertica and Snowflake databases, especially when primary keys are not explicitly defined in Vertica.


---

ðŸ“… Timeline: 15-Day Execution Plan


---

1. Connection Setup (Days 1â€“3)

ðŸ”¹ Description:

Establish secure, production-compatible Python connections to both Vertica and Snowflake environments.

âœ… Tasks:

Setup .env configuration for sensitive variables

Use vertica_python for Vertica connection

Use snowflake-connector-python for Snowflake connection

Validate connections by executing test queries


ðŸŽ¯ Deliverable:

Verified scripts that securely connect and query both databases.


---

2. Comparison Logic Design (Days 4â€“7)

ðŸ”¹ Description:

Design logic for comparing both schema (DDL) and data content, considering the absence of primary keys in Vertica.

âœ… Tasks:

DDL Comparison:

Use information_schema.columns to extract table structure

Compare column names, data types, nullability, and order


Data Comparison:

For tables with known unique identifiers, use FULL OUTER JOIN

For tables without keys, apply:

Row-level SHA256 hashing

Sampling or column-based matching heuristics




ðŸŽ¯ Deliverable:

Reusable logic framework that covers schema and data comparison even without strict keys.


---

3. Pilot Run on Sample Tables (Days 8â€“9)

ðŸ”¹ Description:

Test the entire logic on a small batch of tables to confirm correctness, handle edge cases, and benchmark processing.

âœ… Tasks:

Select 3â€“5 diverse tables

Run DDL and data comparison scripts

Handle mismatches like:

Casing issues (e.g., Snowflake = UPPERCASE)

Data type mismatches

Missing/null value inconsistencies


Log mismatched row count and key insights


ðŸŽ¯ Deliverable:

Initial evidence and refinement opportunity for scaling.


---

4. Scaling the Comparison Approach (Days 10â€“12)

ðŸ”¹ Description:

Prepare the system to handle 400+ tables automatically in batches.

âœ… Tasks:

Loop through metadata tables to generate dynamic queries

Log all results (mismatches, missing rows, schema diffs)

Implement row limit or sampling where tables are very large

Add exclusions (e.g., insert_ts, audit columns)


ðŸŽ¯ Deliverable:

Fully automated Python solution that compares all target tables end-to-end.


---

5. Output Structuring and Finalization (Day 13)

ðŸ”¹ Description:

Design and finalize the format in which results will be exported.

âœ… Tasks:

Structure outputs as:

Summary CSV: table-wise match status

Detailed CSVs: mismatch logs with row-wise details


Tag each row: "MATCHED", "ONLY_IN_VERTICA", "ONLY_IN_SNOWFLAKE", "MISMATCHED"

Maintain clean folder structure for ease of access


ðŸŽ¯ Deliverable:

Human-readable, report-ready outputs suitable for handoff or audit.


---

6. Buffer + Documentation (Days 14â€“15)

ðŸ”¹ Description:

Allocate final days for buffer handling, edge case fixes, and documentation.

âœ… Tasks:

Handle unexpected errors (e.g., connection timeouts, permission issues)

Write documentation for:

Setup steps

Configuration instructions

How to run and interpret results


Package the project in a sharable format


ðŸŽ¯ Deliverable:

Well-documented, maintainable, and ready-to-use comparison pipeline.


---

Final Output Includes:

Comparison summary (summary.csv)

Table-specific mismatch logs

Logs for schema mismatches

Clean codebase with reusable functions and configs

End-to-end run guide with examples



---

Let me know if you want this report exported to Word or PDF format, or if you'd like a Gantt-style visualization too.

