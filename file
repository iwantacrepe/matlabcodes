import os import pandas as pd import snowflake.connector import vertica_python from dotenv import load_dotenv

load_dotenv() SCHEMA = os.getenv("ER1_SCHEMA", "ER1") START_DATETIME = "2020-10-01 00:00:00" END_DATETIME = "2025-06-24 23:59:59"

def connect_to_snowflake(): return snowflake.connector.connect( account=os.getenv("URL_SF").split("//")[-1].split(".snowflakecomputing.com")[0], user=os.getenv("USER_SDL_SF"), private_key_file=os.path.join(os.getenv("DATA_ROOT"), os.getenv("KEYTAB_FILE_SDL_SF")), private_key_file_pwd=os.getenv("PASSWORD_SDL_SF"), database=os.getenv("DB_SDL_SF"), schema=os.getenv("SCHEMA_SDL_SF"), warehouse=os.getenv("WAREHOUSE_SDL_SF"), role=os.getenv("ROLE_SDL_SF"), authenticator="SNOWFLAKE_JWT" )

def connect_to_vertica(): return vertica_python.connect( host=os.getenv("VERTICA_HOST"), port=int(os.getenv("VERTICA_PORT", 5433)), user=os.getenv("VERTICA_USER"), password=os.getenv("VERTICA_PASSWORD"), database=os.getenv("VERTICA_DB"), autocommit=True )

def get_columns(conn, table, dbtype="snowflake"): if dbtype == "snowflake": with conn.cursor() as cur: cur.execute(""" SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = %s AND table_name = %s ORDER BY ordinal_position """, (SCHEMA, table)) return [row[0] for row in cur.fetchall()] else: cur = conn.cursor() cur.execute(""" SELECT column_name FROM v_catalog.columns WHERE table_schema = %s AND table_name = %s ORDER BY ordinal_position """, (SCHEMA, table.lower())) result = [row[0] for row in cur.fetchall()] cur.close() return result

def resolve_columns(requested, actual): actual_map = {col.casefold(): col for col in actual} return [actual_map[col.casefold()] for col in requested if col.casefold() in actual_map]

def build_outer_join_query(table, columns, keys, filter_col=None): select_clause = ", ".join([f'a."{col}" AS "{col}_sf", b."{col}" AS "{col}_vt"' for col in columns]) join_clause = " AND ".join([f'a."{k}" = b."{k}"' for k in keys]) query = f''' SELECT {select_clause} FROM "{SCHEMA}"."{table}" a FULL OUTER JOIN "{SCHEMA}"."{table}" b ON {join_clause} ''' if filter_col: query += f''' WHERE COALESCE(a."{filter_col}", b."{filter_col}") BETWEEN '{START_DATETIME}' AND '{END_DATETIME}' ''' return query

def execute_query_as_df(conn, query): with conn.cursor() as cur: cur.execute(query) data = cur.fetchall() columns = [desc[0] for desc in cur.description] return pd.DataFrame(data, columns=columns)

def format_value(val): return "N/A" if pd.isna(val) else str(val)

def compare_rows(df, columns, table): mismatches = [] for _, row in df.iterrows(): for col in columns: val_sf = row.get(f"{col}_sf") val_vt = row.get(f"{col}_vt") if str(val_sf).strip().casefold() != str(val_vt).strip().casefold(): mismatches.append({ "TableName": table, "ColumnName": col, "ValueInVertica": format_value(val_vt), "ValueInSnowflake": format_value(val_sf) }) return mismatches

def compare_table(table, config_df, sf_conn, vt_conn, summary, all_mismatches): sub = config_df[config_df["Table Name"].str.casefold() == table.casefold()] if (sub["Flag"].str.casefold() == "stage").any(): summary.append(f"{table}: Skipped (Stage flagged)") return

key_cols = sub[sub["Flag"].str.casefold() == "key"]["Column Name"].tolist()
exclude_cols = sub[sub["Flag"].str.casefold() == "exclude"]["Column Name"].tolist()
filter_cols = sub[sub["Flag"].str.casefold() == "filter"]["Column Name"].tolist()
filter_col = filter_cols[0] if filter_cols else None

sf_cols = get_columns(sf_conn, table, dbtype="snowflake")
vt_cols = get_columns(vt_conn, table, dbtype="vertica")

sf_map = {col.casefold(): col for col in sf_cols}
vt_map = {col.casefold(): col for col in vt_cols}
common_keys = list(set(sf_map.keys()) & set(vt_map.keys()))
common_cols = [sf_map[k] for k in common_keys if k in sf_map and k in vt_map]

keys = resolve_columns(key_cols, common_cols)
excludes = resolve_columns(exclude_cols, common_cols) if exclude_cols else []
filters = resolve_columns(filter_cols, common_cols) if filter_cols else []

compare_cols = [col for col in common_cols if col not in keys + excludes]
query = build_outer_join_query(table, compare_cols, keys, filter_col=filter_col)

sf_df = execute_query_as_df(sf_conn, query)
vt_df = execute_query_as_df(vt_conn, query)
merged = sf_df.merge(vt_df, how='outer')
mismatches = compare_rows(merged, compare_cols, table)
all_mismatches.extend(mismatches)

summary.append(f"{table}: Compared | Keys: {keys} | Excludes: {excludes or 'None'} | Filter: {filters or 'None'}")

def main(): config = pd.read_excel("test.xlsx") required_cols = {"Table Name", "Flag", "Column Name"} if not required_cols.issubset(config.columns): raise ValueError("Excel file must have columns: Table Name, Flag, Column Name")

tables = config["Table Name"].dropna().unique()
sf_conn = connect_to_snowflake()
vt_conn = connect_to_vertica()
all_mismatches = []
summary = []

try:
    for table in tables:
        compare_table(table, config, sf_conn, vt_conn, summary, all_mismatches)
finally:
    sf_conn.close()
    vt_conn.close()

print("\n=== Summary ===")
for line in summary:
    print(line)

if all_mismatches:
    df = pd.DataFrame(all_mismatches)
    df = df[["TableName", "ColumnName", "ValueInVertica", "ValueInSnowflake"]]
    df.to_excel("comparison_report.xlsx", index=False)
    print("\nMismatch report saved to comparison_report.xlsx")
else:
    print("\nNo mismatches found.")

if name == "main": main()

