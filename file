import os
import hashlib
import pandas as pd
import snowflake.connector
import vertica_python
from dotenv import load_dotenv

#–– Config ––#
load_dotenv()
SCHEMA       = os.getenv("ER1_SCHEMA", "ER1")
TABLE        = "CLIENT_PROFILE_INFO"
BATCH_SIZE   = 10000  # adjust as needed

#–– Helpers ––#
def normalize(name: str) -> str:
    return name.casefold()

def generate_row_hash(row, cols):
    """MD5 of the concatenated values in cols order."""
    s = "|".join(str(row[c]) for c in cols)
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def connect_to_snowflake():
    conn = snowflake.connector.connect(
        account     = os.getenv("URL_SF").split("//")[-1].split(".snowflakecomputing.com")[0],
        user        = os.getenv("USER_SDL_SF"),
        private_key_file     = os.path.join(os.getenv("DATA_ROOT"), os.getenv("KEYTAB_FILE_SDL_SF")),
        private_key_file_pwd = os.getenv("PASSWORD_SDL_SF"),
        database    = os.getenv("DB_SDL_SF"),
        schema      = os.getenv("SCHEMA_SDL_SF"),
        warehouse   = os.getenv("WAREHOUSE_SDL_SF"),
        role        = os.getenv("ROLE_SDL_SF"),
        authenticator = 'SNOWFLAKE_JWT'
    )
    return conn

def connect_to_vertica():
    return vertica_python.connect(
        host       = os.getenv("VERTICA_HOST"),
        port       = int(os.getenv("VERTICA_PORT", 5433)),
        user       = os.getenv("VERTICA_USER"),
        password   = os.getenv("VERTICA_PASSWORD"),
        database   = os.getenv("VERTICA_DB"),
        autocommit = True
    )

def get_columns(conn, table_name, schema, db="sf"):
    """Fetch column list from INFORMATION_SCHEMA for SF or v_catalog for VT."""
    if db == "sf":
        sql = """
            SELECT column_name
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """
    else:
        sql = """
            SELECT column_name
            FROM v_catalog.columns
            WHERE table_schema = %s AND UPPER(table_name) = UPPER(%s)
            ORDER BY ordinal_position
        """
    cur = conn.cursor()
    cur.execute(sql, (schema, table_name))
    cols = [r[0] for r in cur.fetchall()]
    cur.close()
    return cols

def fetch_batches(conn, table, schema, cols, db="sf"):
    """Yield DataFrames of size BATCH_SIZE ordered by cols."""
    # build ORDER BY list
    order_by = ", ".join(f'"{c}"' if db=="sf" else c for c in cols)
    base = (f'SELECT {", ".join(f\'"{c}"\' if db=="sf" else c for c in cols)} '
            f'FROM {schema}.{table}' if db=="vt"
            else f'SELECT {", ".join(f\'"{c}"\' for c in cols)} FROM "{schema}"."{table}"')
    query = f"{base} ORDER BY {order_by} LIMIT %s OFFSET %s"
    offset = 0

    while True:
        cur = conn.cursor()
        cur.execute(query, (BATCH_SIZE, offset))
        rows = cur.fetchall()
        if not rows:
            cur.close()
            break
        df = pd.DataFrame(rows, columns=cols)
        cur.close()
        yield df
        offset += BATCH_SIZE

def compare_table(sf_conn, vt_conn, table, schema):
    # 1) get column lists and compute common cols in deterministic order
    sf_cols = get_columns(sf_conn, table, schema, db="sf")
    vt_cols = get_columns(vt_conn, table, schema, db="vt")
    common = [c for c in sf_cols if normalize(c) in {normalize(v) for v in vt_cols}]
    if not common:
        print(f"No common columns found for {table}")
        return

    print(f"Comparing {table} on columns: {common}")

    # 2) batch-wise fetch and hash-compare
    batch_num = 0
    sf_batches = fetch_batches(sf_conn, table, schema, common, db="sf")
    vt_batches = fetch_batches(vt_conn, table, schema, common, db="vt")

    mismatches = 0
    while True:
        try:
            sf_df = next(sf_batches)
            vt_df = next(vt_batches)
        except StopIteration:
            break

        # if row counts differ in this batch
        if len(sf_df) != len(vt_df):
            print(f"❗ Batch {batch_num}: row count differs — SF:{len(sf_df)}, VT:{len(vt_df)}")
            mismatches += 1
            break

        # compute hashes
        sf_hash = sf_df.apply(lambda r: generate_row_hash(r, common), axis=1)
        vt_hash = vt_df.apply(lambda r: generate_row_hash(r, common), axis=1)

        # find rows that differ
        diff_idx = sf_hash.ne(vt_hash)
        if diff_idx.any():
            for i in diff_idx[diff_idx].index:
                print(f"\nMismatch in batch {batch_num}, row {i}:")
                print(" SF:", sf_df.loc[i].to_dict())
                print(" VT:", vt_df.loc[i].to_dict())
                mismatches += 1

        batch_num += 1

    if mismatches == 0:
        print("✅ No mismatches found.")
    else:
        print(f"❌ Total mismatches: {mismatches}")

if __name__ == "__main__":
    sf_conn = connect_to_snowflake()
    vt_conn = connect_to_vertica()
    try:
        compare_table(sf_conn, vt_conn, TABLE, SCHEMA)
    finally:
        sf_conn.close()
        vt_conn.close()