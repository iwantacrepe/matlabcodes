from openpyxl import load_workbook
import os
import pandas as pd
import snowflake.connector
import vertica_python
from dotenv import load_dotenv

# Show full DataFrames in console
pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)

load_dotenv()
SCHEMA       = os.getenv("ER1_SCHEMA", "ER1")
START_DATETIME = "2020-10-01 00:00:00"
END_DATETIME   = "2025-06-24 23:59:59"

def connect_to_snowflake():
    return snowflake.connector.connect(
        account=os.getenv("URL_SF").split("//")[-1].split(".snowflakecomputing.com")[0],
        user=os.getenv("USER_SDL_SF"),
        private_key_file=os.path.join(os.getenv("DATA_ROOT"), os.getenv("KEYTAB_FILE_SDL_SF")),
        private_key_file_pwd=os.getenv("PASSWORD_SDL_SF"),
        database=os.getenv("DB_SDL_SF"),
        schema=os.getenv("SCHEMA_SDL_SF"),
        warehouse=os.getenv("WAREHOUSE_SDL_SF"),
        role=os.getenv("ROLE_SDL_SF"),
        authenticator="SNOWFLAKE_JWT"
    )

def connect_to_vertica():
    return vertica_python.connect(
        host=os.getenv("VERTICA_HOST"),
        port=int(os.getenv("VERTICA_PORT", 5433)),
        user=os.getenv("VERTICA_USER"),
        password=os.getenv("VERTICA_PASSWORD"),
        database=os.getenv("VERTICA_DB"),
        autocommit=True
    )

def get_columns(conn, table, dbtype="snowflake"):
    if dbtype == "snowflake":
        with conn.cursor() as cur:
            cur.execute("""
                SELECT column_name
                  FROM INFORMATION_SCHEMA.COLUMNS
                 WHERE table_schema = %s
                   AND table_name    = %s
              ORDER BY ordinal_position
            """, (SCHEMA, table))
            return [r[0] for r in cur.fetchall()]
    else:
        cur = conn.cursor()
        cur.execute("""
            SELECT column_name
              FROM v_catalog.columns
             WHERE table_schema = %s
               AND UPPER(table_name) = UPPER(%s)
          ORDER BY ordinal_position
        """, (SCHEMA, table))
        cols = [r[0] for r in cur.fetchall()]
        cur.close()
        return cols

def resolve_columns(requested, actual, force_upper=False):
    """
    Map requested names (case-insensitive) to actual column names;
    if force_upper, return actual_name.upper().
    """
    amap = {c.casefold(): c for c in actual}
    out = []
    for req in requested:
        key = req.casefold()
        if key not in amap:
            raise ValueError(f"Column '{req}' not found")
        name = amap[key]
        out.append(name.upper() if force_upper else name)
    return out

def fetch_table_df(conn, table, schema, cols, dbtype):
    """
    Pulls the entire table into a DataFrame, renaming cols -> col_sf or col_vt.
    """
    if dbtype == "sf":
        select = ", ".join(f'"{c}"' for c in cols)
        sql    = f'SELECT {select} FROM "{schema}"."{table}"'
        suffix = "_sf"
    else:
        select = ", ".join(cols)
        sql    = f"SELECT {select} FROM {schema}.{table}"
        suffix = "_vt"

    cur = conn.cursor()
    cur.execute(sql)
    rows = cur.fetchall()
    cur.close()

    df = pd.DataFrame(rows, columns=cols)
    return df.rename(columns={c: c + suffix for c in cols})

def format_value(v):
    return "N/A" if pd.isna(v) else str(v)

def compare_rows(df, compare_cols, keys, table):
    """
    Returns list of mismatch dicts for each (row, col) where values differ.
    """
    mismatches = []
    for _, row in df.iterrows():
        for col in compare_cols:
            sf = row.get(f"{col}_sf")
            vt = row.get(f"{col}_vt")
            if format_value(sf).strip().casefold() != format_value(vt).strip().casefold():
                mismatches.append({
                    "TableName":         table,
                    "ColumnName":        col,
                    "ValueInSnowflake":  format_value(sf),
                    "ValueInVertica":    format_value(vt),
                    "Key":                ", ".join(format_value(row.get(k)) for k in keys)
                })
    return mismatches

def compare_table(table, cfg, sf_conn, vt_conn, summary, out_path, first_write):
    sub = cfg[cfg["Table Name"].str.casefold() == table.casefold()]

    # 1) Skip if any row is flagged "stage"
    if (sub["Flag"].str.casefold() == "stage").any():
        summary.append(f"{table}: Skipped (stage flag)")
        return first_write

    # 2) Gather flags
    key_cols     = sub[sub["Flag"].str.casefold()=="key"]["Column Name"].tolist()
    exclude_cols = sub[sub["Flag"].str.casefold()=="exclude"]["Column Name"].tolist()

    # 3) Discover actual columns
    sf_cols = get_columns(sf_conn, table, dbtype="snowflake")
    vt_cols = get_columns(vt_conn, table, dbtype="vertica")

    # 4) Determine common columns
    sf_map = {c.casefold(): c for c in sf_cols}
    vt_map = {c.casefold(): c for c in vt_cols}
    common = set(sf_map) & set(vt_map)
    common_cols = [sf_map[n] for n in common]

    # 5) Remove excludes by casefold
    excl_set = {e.casefold() for e in exclude_cols}
    common_cols = [c for c in common_cols if c.casefold() not in excl_set]

    # 6) Resolve keys (force_upper for Snowflake quoting)
    keys = resolve_columns(key_cols, sf_cols + vt_cols, force_upper=True)

    # 7) Final compare list (common minus keys)
    compare_cols = [c for c in common_cols if c not in keys]

    print(f"\n[INFO] Table: {table}")
    print("  Keys:   ", keys)
    print("  Exclude:", exclude_cols)
    print("  Compare:", compare_cols)

    # 8) Fetch once
    fetch_cols = keys + compare_cols
    sf_df = fetch_table_df(sf_conn, table, SCHEMA, fetch_cols, dbtype="sf")
    vt_df = fetch_table_df(vt_conn, table, SCHEMA, fetch_cols, dbtype="vt")

    # 9) Merge on keys
    left_on  = [k + "_sf" for k in keys]
    right_on = [k + "_vt" for k in keys]
    merged = pd.merge(sf_df, vt_df, how="outer", left_on=left_on, right_on=right_on)

    # 10) Reconstruct key columns for output
    for k in keys:
        merged[k] = merged[f"{k}_sf"].fillna(merged[f"{k}_vt"])

    # 11) Compare
    mismatches = compare_rows(merged, compare_cols, keys, table)

    # 12) Write Excel
    if mismatches:
        dfm = pd.DataFrame(mismatches)[["TableName","ColumnName","ValueInSnowflake","ValueInVertica","Key"]]
        if first_write:
            dfm.to_excel(out_path, index=False, engine="openpyxl")
            first_write = False
        else:
            book = load_workbook(out_path)
            with pd.ExcelWriter(out_path, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
                w.book   = book
                w.sheets = {ws.title: ws for ws in book.worksheets}
                start = w.sheets["Sheet1"].max_row
                dfm.to_excel(w, index=False, header=False, startrow=start)
        summary.append(f"{table}: {len(mismatches)} mismatches")
    else:
        summary.append(f"{table}: No mismatches")

    return first_write

def main():
    config     = pd.read_excel("test.xlsx")
    required   = {"Table Name", "Flag", "Column Name"}
    if not required.issubset(config.columns):
        raise ValueError("test.xlsx must have: Table Name, Flag, Column Name")

    tables     = config["Table Name"].dropna().unique()
    sf_conn    = connect_to_snowflake()
    vt_conn    = connect_to_vertica()
    summary    = []
    output_path = "comparison_report.xlsx"

    if os.path.exists(output_path):
        os.remove(output_path)
    first_write = True

    try:
        for tbl in tables:
            first_write = compare_table(tbl, config, sf_conn, vt_conn, summary, output_path, first_write)
    finally:
        sf_conn.close()
        vt_conn.close()

    print("\n=== Summary ===")
    print("\n".join(summary))
    print("\nReport:", output_path if os.path.exists(output_path) else "No mismatches")

if __name__ == "__main__":
    main()