Of course. Here is your Python script with detailed comments explaining the logic and the purpose behind each section. This will help the next developer understand, maintain, and extend the code.
Code Overview
This script performs a data validation and comparison between Vertica (source) and Snowflake (target) databases. Its primary goal is to identify discrepancies in data for a given set of tables.
How it works:
 * Configuration: It reads an Excel file (table.xlsx) that specifies which tables to compare. For each table, the file defines key columns (for joining), columns to exclude from comparison, and an optional filter column (e.g., a timestamp for incremental checks).
 * Connection: It connects to both Vertica and Snowflake using credentials stored as environment variables. It uses a separate connection for writing results to avoid transaction conflicts.
 * Batch Processing: For each table, it fetches data from both databases in manageable chunks (batches). The batch size is dynamically adjusted based on the number of columns to prevent memory overload.
 * Comparison: It compares the data row-by-row and column-by-column. It checks for:
   * Rows that exist in one database but not the other.
   * Mismatched values in common columns.
 * Logging: All discrepancies are written into a specific Vertica table (ER1.vertica_snowflake_table_data_comparison) for later analysis.
 * Summary: After processing all tables, it prints a summary to the console.
Commented Code
# =========================================================================================
#                    VERTICA TO SNOWFLAKE DATA VALIDATION SCRIPT
# =========================================================================================
#
# OVERVIEW:
# This script is designed to compare data between tables in a Vertica database (source)
# and a Snowflake database (target). It reads a configuration from an Excel file to
# determine which tables and columns to compare. Any identified discrepancies are
# logged into a results table in Vertica for review.
#
# LIBRARIES USED:
# - pandas: For data manipulation, reading the config file, and handling data in DataFrames.
# - snowflake.connector: The official Python driver for connecting to Snowflake.
# - vertica_python: The Python driver for connecting to Vertica.
# - os: Used to construct file paths for credentials.
# - datetime: Used for handling date/time values, particularly for filtering.
#
# EXECUTION FLOW:
# 1. Load constants and configuration from environment variables and a local Excel file.
# 2. Establish connections to Snowflake and Vertica.
# 3. Clean up old results from the logging table in Vertica.
# 4. Loop through each table specified in the Excel configuration.
# 5. For each table:
#    a. Fetch data in batches from both databases.
#    b. Compare the batches row by row.
#    c. Insert any found mismatches into the Vertica results table.
# 6. Close all database connections.
# 7. Print a final summary of the comparison results.
#
# =========================================================================================

import os
import pandas as pd
import snowflake.connector
import vertica_python
from datetime import datetime

# --- CONFIGURATION & CONSTANTS ---
# These variables are expected to be set as environment variables in the execution
# environment (e.g., in a Docker container, Jenkins job, or shell profile).
# This approach avoids hardcoding sensitive credentials directly in the script.

# Vertica Production Credentials
VERTICA_HOST = VERTICA_HOST
VERTICA_USER = VERTICA_USER
VERTICA_PASSWORD = VERTICA_PASSWORD
VERTICA_DB = VERTICA_DB

# Snowflake Credentials
URL_SF = URL_SF
KEYTAB_FILE_SDL_SF = KEYTAB_FILE_SDL_SF # Private key file for JWT authentication
PASSWORD_SDL_SF = PASSWORD_SDL_SF       # Passphrase for the private key
DB_SDL_SF = DB_SDL_SF
DB_SDL_SF_VW = DB_SDL_SF_VW
SCHEMA_SDL_SF = SCHEMA_SDL_SF
USER_SDL_SF = USER_SDL_SF
WAREHOUSE_SDL_SF = WAREHOUSE_SDL_SF
ROLE_SDL_SF = ROLE_SDL_SF
KEYTAB_DIR = KEYTAB_DIR                 # Directory where the private key is stored
DATA_DIR_VAR = DATA_DIR_VAR,
SNOWFLAKE_JWT = "SNOWFLAKE_JWT"         # Specifies the JWT authentication method

# --- FILE & PATH CONFIGURATION ---
FILE_NAME = "table.xlsx"
XLS_PATH_TABLE = DATA_DIR_VAR[0] + FILE_NAME # Full path to the input configuration Excel file

# --- PANDAS DISPLAY SETTINGS ---
# These options configure pandas to display all rows and columns of a DataFrame
# when printed to the console, which is useful for debugging.
pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)

# --- SCRIPT-LEVEL PARAMETERS ---
SCHEMA = "ER1"  # The database schema to work within for both Vertica and Snowflake.
START_DATETIME = "2020-10-01 00:00:00" # A default start date for incremental filtering.

# Parameters for dynamic batch size calculation.
BASELINE_BATCH_SIZE = 100000  # The batch size for a table with a standard number of columns.
BASELINE_COLS = 50            # The standard number of columns used as a baseline.
MIN_BATCH_SIZE = 10000        # The absolute minimum batch size to use, regardless of column count.

# A default list of columns to always exclude from comparison (e.g., ETL metadata).
DEFAULT_EXCLUDE_COLS = [
    "INSRT_TS", "UPDT_TS", "ETL_TRNS", "ETL_JOB", "INSRT_TR", "INSRT_JB",
    "CREATION_DATE_TIME", "UPDT_TR", "UPDT_JB", "LAST_UPDATED_DATE"
]

def calculate_dynamic_batch_size(num_columns):
    """
    Calculates batch size dynamically based on the number of columns to fetch.
    The goal is to manage memory usage by fetching fewer rows when there are many columns.
    This keeps the total data points per batch (rows * columns) relatively consistent.
    """
    if num_columns <= 0:
        return BASELINE_BATCH_SIZE  # Return default if column count is invalid.

    # Calculate the target "data load" based on our baseline constants.
    target_data_load = BASELINE_BATCH_SIZE * BASELINE_COLS

    # Calculate the ideal batch size for the given number of columns.
    dynamic_size = int(target_data_load / num_columns)
    
    # Ensure the calculated size is within our defined min/max bounds.
    return max(MIN_BATCH_SIZE, min(dynamic_size, BASELINE_BATCH_SIZE))

def is_datetime_type(dtype):
    """
    Checks if a column's data type string represents a date, time, or timestamp.
    This is used to apply special formatting (casting to DATE) to these columns
    to avoid comparison issues related to timezones or fractional seconds.
    """
    if not isinstance(dtype, str):
        return False
    dtype_upper = dtype.upper()
    return 'TIMESTAMP' in dtype_upper or 'DATE' in dtype_upper or 'TIME' in dtype_upper

def connect_to_snowflake():
    """
    Establishes and returns a connection to Snowflake using private key authentication.
    """
    print("Connecting to Snowflake...")
    return snowflake.connector.connect(
        account=URL_SF.split("//")[-1].split(".snowflakecomputing.com")[0], # Extracts the account name from the URL
        user=USER_SDL_SF,
        private_key_file=os.path.join(KEYTAB_DIR, KEYTAB_FILE_SDL_SF), # Full path to the private key
        private_key_file_pwd=PASSWORD_SDL_SF, # Passphrase for the key
        database=DB_SDL_SF,
        schema=SCHEMA_SDL_SF,
        warehouse=WAREHOUSE_SDL_SF,
        role=ROLE_SDL_SF,
        authenticator=SNOWFLAKE_JWT
    )

def connect_to_vertica():
    """
    Establishes and returns a connection to the production Vertica database.
    """
    print("Connecting to Vertica...")
    return vertica_python.connect(
        host=VERTICA_HOST,
        port=5433,
        user=VERTICA_USER,
        password=VERTICA_PASSWORD,
        database=VERTICA_DB,
        autocommit=True  # Ensures each statement is committed automatically.
    )

def get_snowflake_columns_and_types(conn, table):
    """
    Fetches column names and their data types for a given table from Snowflake's
    metadata catalog (INFORMATION_SCHEMA).
    Returns a dictionary like {'COLUMN_NAME': 'DATA_TYPE'}.
    """
    with conn.cursor() as cur:
        # This query retrieves metadata directly from Snowflake.
        cur.execute("""
            SELECT column_name, data_type
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE table_schema = %s AND UPPER(table_name) = %s
            ORDER BY ordinal_position
        """, (SCHEMA, table.upper()))
        return {r[0]: r[1] for r in cur.fetchall()} # Convert result to a dictionary

def get_vertica_columns_and_types(conn, table):
    """
    Fetches column names and their data types for a given table from Vertica's
    metadata catalog (v_catalog).
    Returns a dictionary like {'COLUMN_NAME': 'DATA_TYPE'}.
    """
    with conn.cursor() as cur:
        # This query retrieves metadata directly from Vertica.
        cur.execute("""
            SELECT column_name, data_type
            FROM v_catalog.columns
            WHERE table_schema = %s AND LOWER(table_name) = %s
            ORDER BY ordinal_position
        """, (SCHEMA, table.lower()))
        return {r[0]: r[1] for r in cur.fetchall()} # Convert result to a dictionary

def get_row_count(conn, table, schema, filter_col=None, start_datetime=None):
    """
    Gets the total row count for a table. If a filter column and start date are
    provided, it gets the count for the filtered subset. This is a quick check
    to see if tables are empty before attempting a full comparison.
    """
    query = f'SELECT COUNT(*) FROM "{schema}"."{table}"'
    params = ()
    # If a filter is specified, add a WHERE clause to the query.
    if filter_col and start_datetime:
        query += f' WHERE "{filter_col}" >= %s'
        params = (start_datetime,)

    with conn.cursor() as cur:
        cur.execute(query, params)
        return cur.fetchone()[0] # Returns the first value of the first row (the count).

def resolve_columns(requested, actual, force_upper=False):
    """
    Resolves a list of requested column names against a list of actual available
    column names from the database, performing a case-insensitive match.
    This is crucial because column casing can differ between databases or in the config file.
    """
    resolved = []
    # Create a mapping of lowercase actual column names to their original casing.
    actual_map = {col.casefold(): col for col in actual}
    for col in requested:
        # Check if the lowercase version of the requested column exists in our map.
        if col.casefold() not in actual_map:
            raise ValueError(f"Configuration Error: Column '{col}' not found in the table.")
        # Get the actual column name with its original casing.
        resolved_name = actual_map[col.casefold()]
        if force_upper:
            resolved_name = resolved_name.upper()
        resolved.append(resolved_name)
    return resolved

def trim_to_bytes(s, max_bytes, encoding='utf-8'):
    """
    Trims a string to a maximum number of bytes, safely handling multi-byte
    characters (like emojis or special symbols) to prevent cutting a character in half.
    """
    encoded_s = s.encode(encoding)
    if len(encoded_s) <= max_bytes:
        return s
    # Trim the byte representation and decode it, ignoring errors from incomplete characters.
    return encoded_s[:max_bytes].decode(encoding, errors='ignore')

def format_value(val, max_db_bytes=255):
    """
    Formats a value before inserting it into the results table. It converts the value
    to a string and trims it to a max byte length to prevent database errors (e.g.,
    'value too long for type character varying(255)').
    """
    if pd.isna(val):
        return "NULL" # Represent pandas Not a Number/Null as a string "NULL".
    val_str = str(val)
    return trim_to_bytes(val_str, max_db_bytes)

def is_empty(val):
    """A helper function to check if a value is null, NaN, or just an empty/whitespace string."""
    return pd.isna(val) or str(val).strip() == ""

def compare_rows(df, compare_cols, keys, table):
    """
    Compares rows in a merged DataFrame (containing data from both Snowflake and Vertica)
    and identifies any discrepancies.
    """
    mismatches = []
    # Iterate through each row of the merged DataFrame.
    for _, row in df.iterrows():
        # Construct a key string for logging purposes (e.g., "ID=123, CODE=ABC").
        row_keys = ", ".join([
            str(row.get(f"{k}_sf")) if not pd.isna(row.get(f"{k}_sf"))
            else str(row.get(f"{k}_vt")) for k in keys
        ])

        # --- Check for rows that are missing entirely from one source ---
        # This happens when the 'outer' merge results in all columns from one source being null.
        is_entirely_missing_in_vt = all(is_empty(row.get(f"{col}_vt")) for col in compare_cols) and any(not is_empty(row.get(f"{col}_sf")) for col in compare_cols)
        is_entirely_missing_in_sf = all(is_empty(row.get(f"{col}_sf")) for col in compare_cols) and any(not is_empty(row.get(f"{col}_vt")) for col in compare_cols)
        
        # Log if the row is missing in Vertica.
        if is_entirely_missing_in_vt:
            mismatches.append({
                "TableName": table, "ColumnName": "Row is Missing",
                "ValueInVertica": "Missing in Vertica", "ValueInSnowflake": "Present",
                "Key": row_keys
            })
            continue # Move to the next row

        # Log if the row is missing in Snowflake.
        if is_entirely_missing_in_sf:
            mismatches.append({
                "TableName": table, "ColumnName": "Row is Missing",
                "ValueInVertica": "Present", "ValueInSnowflake": "Missing in Snowflake",
                "Key": row_keys
            })
            continue # Move to the next row

        # --- Check for mismatched values in individual columns ---
        for col in compare_cols:
            val_sf = row.get(f"{col}_sf")
            val_vt = row.get(f"{col}_vt")

            # If both values are null, they match, so we can skip.
            if pd.isna(val_sf) and pd.isna(val_vt):
                continue

            # A mismatch occurs if:
            # 1. One value is null but the other is not.
            # 2. Neither are null, but their string representations (trimmed and lowercased) are different.
            if pd.isna(val_sf) != pd.isna(val_vt) or (
                not pd.isna(val_sf) and not pd.isna(val_vt) and
                str(val_sf).strip().casefold() != str(val_vt).strip().casefold()
            ):
                mismatches.append({
                    "TableName": table, "ColumnName": col,
                    "ValueInVertica": format_value(val_vt),
                    "ValueInSnowflake": format_value(val_sf),
                    "Key": row_keys
                })

    return mismatches

def compare_table(table, config_df, sf_conn, vt_conn, vt_conn_2, summary):
    """
    This is the main orchestration function for comparing a single table.
    It handles configuration, pre-checks, batching, and invoking the comparison logic.
    """
    print(f"--- Starting comparison for table: {table} ---")
    
    # --- 1. Read Configuration for this Specific Table ---
    # Filter the main config DataFrame to get settings for the current table.
    sub = config_df[config_df["Table Name"].str.casefold() == table.casefold()]
    
    # Check for the 'stage' flag, which means we should skip this table entirely.
    if (sub["Flag"].str.casefold() == "stage").any():
        summary.append(f"Table '{table}': Skipped (Flagged as 'stage')")
        print(f"Skipping table '{table}' as it is flagged as 'stage'.")
        return

    # Extract lists of key, exclude, and filter columns from the config.
    key_cols = sub[sub["Flag"].str.casefold() == "key"]["Column Name"].tolist()
    exclude_cols = sub[sub["Flag"].str.casefold() == "exclude"]["Column Name"].tolist()
    filter_cols = sub[sub["Flag"].str.casefold() == "filter"]["Column Name"].tolist()
    filter_col = filter_cols[0] if filter_cols else None
    
    if filter_col:
        print(f"[INFO] Applying filter on column '{filter_col}' >= '{START_DATETIME}'.")

    # A table must have key columns defined to be comparable.
    if not key_cols:
        summary.append(f"Table '{table}': Skipped (No key columns defined in config)")
        print(f"[ERROR] Skipping table '{table}' because no key columns were defined in the config.")
        return
    
    # --- 2. Perform Pre-Checks (Row Counts) ---
    print(f"[INFO] Performing initial row count check for table '{table}'...")
    sf_count = get_row_count(sf_conn, table, SCHEMA, filter_col, START_DATETIME)
    vt_count = get_row_count(vt_conn, table, SCHEMA, filter_col, START_DATETIME)
    print(f"[INFO] Row Counts -> Snowflake: {sf_count}, Vertica: {vt_count}")

    # Handle cases where one or both tables are empty.
    data_to_insert = None
    if sf_count == 0 and vt_count > 0:
        summary.append(f"Table '{table}': Logged as empty in Snowflake (Vertica has {vt_count} rows)")
        print(f"[!!] Table '{table}' is empty in Snowflake but not in Vertica. Logging and skipping detailed comparison.")
        data_to_insert = [(table, 'EmptyTable', f'{vt_count} rows', 'Empty', 'Table is empty in Snowflake')]
    elif vt_count == 0 and sf_count > 0:
        summary.append(f"Table '{table}': Logged as empty in Vertica (Snowflake has {sf_count} rows)")
        print(f"[!!] Table '{table}' is empty in Vertica but not in Snowflake. Logging and skipping detailed comparison.")
        data_to_insert = [(table, 'EmptyTable', 'Empty', f'{sf_count} rows', 'Table is empty in Vertica')]
    elif sf_count == 0 and vt_count == 0:
        summary.append(f"Table '{table}': Skipped (Empty in both sources)")
        print(f"Skipping table '{table}' as it's empty in both sources.")
        return

    # If we found an empty-table discrepancy, insert it and stop processing this table.
    if data_to_insert:
        insert_query = """
            INSERT INTO ER1.vertica_snowflake_table_data_comparison
            (table_name, column_name, value_in_vertica, value_in_snowflake, unique_key)
            VALUES (%s, %s, %s, %s, %s)
        """
        with vt_conn_2.cursor() as cur:
            cur.executemany(insert_query, data_to_insert)
        return
        
    # --- 3. Resolve Column Names and Define Comparison Scope ---
    # Get column metadata from both databases.
    sf_cols_map = get_snowflake_columns_and_types(sf_conn, table)
    vt_cols_map = get_vertica_columns_and_types(vt_conn, table)
    
    sf_cols = list(sf_cols_map.keys())
    vt_cols = list(vt_cols_map.keys())

    # Find the set of common columns (case-insensitive).
    sf_norm_map = {col.casefold(): col for col in sf_cols}
    vt_norm_map = {col.casefold(): col for col in vt_cols}
    common_norm = set(sf_norm_map.keys()) & set(vt_norm_map.keys())
    common_cols = [sf_norm_map[n] for n in common_norm] # Get original casing
    all_cols = list(set(sf_cols + vt_cols)) # All unique columns from both sources

    # Resolve the final lists of keys and columns to exclude.
    keys = resolve_columns(key_cols, all_cols, force_upper=True)
    excludes_from_excel = resolve_columns(exclude_cols, all_cols, force_upper=True) if exclude_cols else []
    final_excludes = list(set(excludes_from_excel) | set(DEFAULT_EXCLUDE_COLS)) # Combine default and Excel excludes
    
    # The final list of columns to compare is what's common, minus keys and excludes.
    compare_cols = [col for col in common_cols if col.upper() not in keys + final_excludes]
    
    # --- 4. Set up Batch Processing ---
    total_cols_to_fetch = len(keys) + len(compare_cols)
    dynamic_batch_size = calculate_dynamic_batch_size(total_cols_to_fetch)
    print(f"[INFO] Using dynamic batch size: {dynamic_batch_size} for {total_cols_to_fetch} total columns.")

    def fetch_df_batch(conn, dbtype, limit, offset):
        """
    