import dask.dataframe as dd

csv_path = "large_file.csv"
output_dir = "output_parquet_chunks"

# Read in partitions
ddf = dd.read_csv(csv_path, blocksize="64MB")  # Creates small chunks

# Process and write one partition at a time (like manual loop)
for i in range(ddf.npartitions):
    part = ddf.get_partition(i)
    part.to_parquet(f"{output_dir}/part_{i}.parquet", engine="pyarrow", write_index=False, compression="snappy", compute=True)