#!/usr/bin/env python3
import os
import argparse
import pandas as pd
import snowflake.connector
import vertica_python
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()
SCHEMA = os.getenv("ER1_SCHEMA", "ER1")

def connect_to_snowflake():
    return snowflake.connector.connect(
        account   = os.getenv("URL_SF").split("//")[-1].split(".snowflakecomputing.com")[0],
        user      = os.getenv("USER_SDL_SF"),
        private_key_file     = os.path.join(os.getenv("DATA_ROOT"), os.getenv("KEYTAB_FILE_SDL_SF")),
        private_key_file_pwd = os.getenv("PASSWORD_SDL_SF"),
        database  = os.getenv("DB_SDL_SF"),
        schema    = os.getenv("SCHEMA_SDL_SF"),
        warehouse = os.getenv("WAREHOUSE_SDL_SF"),
        role      = os.getenv("ROLE_SDL_SF"),
        authenticator="SNOWFLAKE_JWT"
    )

def connect_to_vertica():
    return vertica_python.connect(
        host      = os.getenv("VERTICA_HOST"),
        port      = int(os.getenv("VERTICA_PORT", 5433)),
        user      = os.getenv("VERTICA_USER"),
        password  = os.getenv("VERTICA_PASSWORD"),
        database  = os.getenv("VERTICA_DB"),
        autocommit=True
    )

def format_value(v):
    return "N/A" if pd.isna(v) else str(v)

def compare_table(table, cfg, start_date, sf_conn, vt_conn, out_records):
    sub = cfg[cfg['Table Name']==table]
    # skip if Stage
    if (sub['Flag']=='Stage').any():
        return

    keys    = sub.loc[sub['Flag']=='Key',    'Column Name'].tolist()
    excludes= sub.loc[sub['Flag']=='Exclude','Column Name'].tolist()
    filters = sub.loc[sub['Flag']=='Filter', 'Column Name'].tolist()

    if filters and not start_date:
        raise ValueError(f"{table}: has Filter column(s) {filters} but no --start-date given")

    # build queries
    sf_q = f'SELECT * FROM "{SCHEMA}"."{table}"'
    vt_q = f'SELECT * FROM {SCHEMA}.{table}'
    params = []
    if filters:
        # assume only one filter column
        col = filters[0]
        sf_q += f' WHERE "{col}" >= %s'
        vt_q += f' WHERE {col} >= %s'
        params = [ start_date ]

    # load into pandas
    sf_df = pd.read_sql(sf_q, sf_conn, params=params)
    vt_df = pd.read_sql(vt_q, vt_conn, params=params)

    # full outer join
    merged = sf_df.merge(vt_df, on=keys, how='outer', suffixes=('_sf','_vt'))

    # compare all columns except keys and excludes
    common_cols = [c for c in set(sf_df.columns).intersection(vt_df.columns)
                   if c not in keys + excludes]
    for col in common_cols:
        a = merged[f"{col}_sf"]
        b = merged[f"{col}_vt"]
        mask = (
            ( a.isna() & ~b.isna() ) |
            ( ~a.isna() & b.isna() ) |
            ( a.notna() & b.notna() & (a != b) )
        )
        mism = merged[mask]
        for _, row in mism.iterrows():
            # extra rows: if key columns all-NaN on one side
            vsf = row[f"{col}_sf"]
            vvt = row[f"{col}_vt"]
            out_records.append({
                "TableName":          table,
                "KeyValues":          {k: format_value(row[k]) for k in keys},
                "ColumnName":         col,
                "ValueInSnowflake":   format_value(vsf),
                "ValueInVertica":     format_value(vvt)
            })

def main():
    p = argparse.ArgumentParser(
        description="Compare Snowflake vs Vertica using a mapping file"
    )
    p.add_argument("mapping_file",
        help="Excel or CSV with columns: Table Name, Flag, Column Name")
    p.add_argument("--start-date", "-d",
        help="YYYY-MM-DD start date for any Filter columns")
    args = p.parse_args()

    if args.start_date:
        # validate
        try:
            start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
        except ValueError:
            p.error("Invalid --start-date, must be YYYY-MM-DD")
    else:
        start_date = None

    # read mapping
    if args.mapping_file.lower().endswith(".csv"):
        cfg = pd.read_csv(args.mapping_file)
    else:
        cfg = pd.read_excel(args.mapping_file)

    # sanity
    expected = {'Table Name','Flag','Column Name'}
    if not expected.issubset(set(cfg.columns)):
        p.error(f"mapping file must have columns: {expected}")

    tables = cfg['Table Name'].unique()
    sf_conn = connect_to_snowflake()
    vt_conn = connect_to_vertica()
    out = []

    try:
        for tbl in tables:
            compare_table(tbl, cfg, start_date, sf_conn, vt_conn, out)
    finally:
        sf_conn.close()
        vt_conn.close()

    if out:
        df = pd.DataFrame(out)
        df.to_excel("comparison_report.xlsx", index=False)
        print(f"Found {len(df)} mismatches â†’ comparison_report.xlsx")
    else:
        print("No mismatches found.")

if __name__=="__main__":
    main()